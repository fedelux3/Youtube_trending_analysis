\documentclass[10pt, a4paper,openany]{article}
\usepackage[italian]{babel}
\usepackage[T1]{fontenc}
\usepackage[table]{xcolor}
\usepackage{float}
\restylefloat{table,figure}
\usepackage{graphicx}	
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{multicol}
\usepackage{fancyhdr}
\usepackage{geometry}
\geometry{a4paper,top=2cm,bottom=2cm,left=3cm,right=3cm,%
	heightrounded,bindingoffset=5mm}

\usepackage{amssymb}
\usepackage{amsthm}


\begin{document}
\begin{center}
\textbf{Analisi delle correlazioni tra le tendenze di youtube e quelle di instagram}
\end{center}

\begin{center}
Gabriele \textbf{inserire cognome}, Federico Luzzi,  Marco Peracchi , Christian Uccheddu
\end{center}

\hrule
\vspace{0.5cm}

\centering\textbf{{Introduzione e obiettivi}}
\\
Questo lavoro si occupa di analizzare le correlazioni presenti tra le tendenze di youtube e quelle di instagram nel tempo. In particolare attraverso l'uso di indicatori creati ad hoc ci si è occupati di cercare il prototipo di personaggio con video ideale per un dato periodo dell'anno, in questo caso le festività natalizie.
\vspace{0.5cm}
\hrule

\flushleft
\section*{Raccolta dati}

In questa sezione del report verrà esposta la procedura con cui sono stati acquisiti i dati e come sono state affrontate le problematiche riscontrate.
\subsection*{Scelta strumenti}
Per prima cosa ci siamo dovuti occupare della scelta degli strumenti più adatti per effettuare la nostra presa dati. Per farlo abbiamo dovuto per prima cosa capire su quali V dei Big Data il nostro progetto sarebbe andato a focalizzarsi. Nel nostro caso è risultato subito evidente come fossero la V di Velocity e quella di Volume. In questo caso abbiamo deciso di utilizzare un database NoSQL quale MongoDB per lo storage dei dati. Il fatto che tramite le API di Youtube riuscissimo a scaricare i dati in formato JSON ci ha indotti ad utilizzare MongoDB che è pensato apposta per i database document based. Per rendere ottimizzato questo procedimento abbiamo usato due script python separati: \textit{scraper\_consumer.py} e \textit{scraper\_producer.py.} In particolzare abbiamo così effettuato una presa dati che può risultare efficace anche con una raccolta dati di volume maggiore. Il nostro oggetto producer si occupa infatti di scaricare i dati da Youtbue attraverso le API

\subsection*{Qualità dati}
\textit{Mostrare le analisi di qualità effettuate sul dataset} Esempio: c'erano missing values? se sì come abbiamo deciso di replicarli?

\subsection*{Scalabilità dell'algoritmo}

Visto che una delle V che abbiamo deciso di usare concernenti i Big Data è stata quella relativa al volume dobbiamo occuparci di vedere come si comporta la nostra elaborazione dati con volumi di dati sempre crescenti. Per farlo ci occupiamo di effettuare la nostra analisi su partizionamenti sempre maggiori del nostro dataset e registriamo il tempo di esecuzione del nostro programma, effettuiamo poi una semplice regressione per vedere di che tipo di crescita stiamo parlando, ovviamente più la crescita è minore più il nostro algoritmo scala bene per volumi di dati maggiori
\section*{Visualizzazione}

\textit{Inserire qua le domande di ricerca e come abbiamo pensato di rispondere a tali domande.}

\subsection*{Scelta features}
\textit{Inserire qua le scelte relative alle caratteristiche del nostro dataset da usare e i motivi che ci hanno spinto a usare proprio quelle .}
\subsection*{Scelta della visualizzazione}
\textit{Inserire qua le scelte relative alla visualizzazione proposta e perché abbiamo deciso di usare proprio quelle.}
\subsection*{Valutazione della qualità}
\textit{inserire le problematiche emerse durante la trasposizione delle infografiche e i metodi usati per risolverle.}

\end{document}